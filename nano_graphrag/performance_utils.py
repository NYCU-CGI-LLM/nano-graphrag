import time
from typing import Dict, List, Union, Any

import tiktoken


class RAGPerformanceTracker:
    """A utility class for calculating and tracking token usage and time
    performance in RAG systems."""

    def __init__(self, model_name: str = "gpt-4o-mini"):
        """Initialize RAGPerformanceTracker

        Args:
            model_name: Model name for tiktoken encoding
        """
        self.model_name = model_name
        self.encoder = tiktoken.encoding_for_model(model_name)

        # Token statistics
        self.token_stats = {
            "query_tokens": 0,                # Number of tokens in user query
            "retrieval_prompt_tokens": 0,     # Number of tokens in retrieval prompts
            "retrieved_data_tokens": 0,       # Number of tokens in retrieved data
            "inference_input_tokens": 0,      # Total input tokens sent to LLM for inference
            "inference_output_tokens": 0      # Number of output tokens generated by LLM
        }

        # Time statistics
        self.time_stats = {
            "retrieve_time": 0,    # Retrieval time
            "llm_time": 0,         # LLM inference time
            "total_time": 0        # Total time
        }

        # Timestamp records
        self._timestamps = {
            "start": 0,
            "retrieve_start": 0,
            "retrieve_end": 0,
            "llm_start": 0,
            "llm_end": 0,
            "end": 0
        }

    def count_tokens(self, text: str) -> int:
        """Count the number of tokens in a text"""
        if not text:
            return 0
        return len(self.encoder.encode(text))

    def count_tokens_batch(self, texts: List[str]) -> List[int]:
        """Batch count tokens for multiple texts"""
        return [self.count_tokens(text) for text in texts]

    def update_query_tokens(self, query: str) -> None:
        """Update the token count for user query"""
        self.token_stats["query_tokens"] = self.count_tokens(query)

    def update_retrieval_prompt_tokens(self, prompt_or_token_count):
        """更新检索提示的token数量
        
        参数可以是提示字符串或直接的token数量
        """
        if isinstance(prompt_or_token_count, int):
            # 如果是整数，直接使用这个值
            self.token_stats["retrieval_prompt_tokens"] = prompt_or_token_count
        else:
            # 如果是字符串，计算token数量
            self.token_stats["retrieval_prompt_tokens"] = self.count_tokens(prompt_or_token_count)

    def update_retrieval_prompt_tokens_batch(self, prompts: List[str]) -> int:
        """Batch update token count for multiple prompts used in retrieval"""
        total_tokens = sum(self.count_tokens_batch(prompts))
        self.token_stats["retrieval_prompt_tokens"] += total_tokens
        return total_tokens

    def update_retrieved_data_tokens(self, data: Union[str, List[str]]) -> int:
        """Update token count for retrieved data
        
        Args:
            data: Retrieved text data, can be a single string or list of strings
            
        Returns:
            Calculated token count
        """
        if isinstance(data, str):
            tokens = self.count_tokens(data)
        else:
            tokens = sum(self.count_tokens_batch(data))

        # Accumulate as there may be multiple retrievals
        self.token_stats["retrieved_data_tokens"] += tokens
        return tokens

    def update_inference_input_tokens(self, full_prompt: str) -> None:
        """Update token count for input sent to LLM for inference
        
        Args:
            full_prompt: Complete LLM input prompt, including system prompt,
                         retrieved data, and user query
        """
        self.token_stats["inference_input_tokens"] = self.count_tokens(
            full_prompt)

    def update_inference_output_tokens(self, completion: str) -> None:
        """Update token count for output generated by LLM"""
        self.token_stats["inference_output_tokens"] = self.count_tokens(
            completion)

    # Time measurement methods
    def start_tracking(self) -> None:
        """Start timing total time"""
        self._timestamps["start"] = time.time_ns()

    def start_retrieve_time(self) -> None:
        """Start timing retrieval time"""
        self._timestamps["retrieve_start"] = time.time_ns()

    def end_retrieve_time(self) -> None:
        """End timing retrieval time and calculate duration"""
        self._timestamps["retrieve_end"] = time.time_ns()
        self.time_stats["retrieve_time"] = (
            self._timestamps["retrieve_end"] -
            self._timestamps["retrieve_start"]
        ) / 1e9

    def start_llm_time(self) -> None:
        """Start timing LLM inference time"""
        self._timestamps["llm_start"] = time.time_ns()

    def end_llm_time(self) -> None:
        """End timing LLM inference time and calculate duration"""
        self._timestamps["llm_end"] = time.time_ns()
        self.time_stats["llm_time"] = (
            self._timestamps["llm_end"] -
            self._timestamps["llm_start"]
        ) / 1e9

    def end_tracking(self) -> None:
        """End timing total time and calculate duration"""
        self._timestamps["end"] = time.time_ns()
        self.time_stats["total_time"] = (
            self._timestamps["end"] -
            self._timestamps["start"]
        ) / 1e9

    def get_token_stats(self) -> Dict[str, int]:
        """Get token statistics"""
        return self.token_stats.copy()

    def get_time_stats(self) -> Dict[str, float]:
        """Get time statistics"""
        return self.time_stats.copy()

    def get_all_stats(self) -> Dict[str, Any]:
        """Get all statistics"""
        return {
            "token_stats": self.token_stats.copy(),
            "time_stats": self.time_stats.copy()
        }

    def reset_stats(self) -> None:
        """Reset all statistics"""
        for key in self.token_stats:
            self.token_stats[key] = 0

        for key in self.time_stats:
            self.time_stats[key] = 0

        for key in self._timestamps:
            self._timestamps[key] = 0
